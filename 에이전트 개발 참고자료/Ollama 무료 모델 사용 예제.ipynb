{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7a12fd-c235-4b0c-8048-d260021738c0",
   "metadata": {},
   "source": [
    "## LLM 모델\n",
    "LangChain에서 LLM은 대규모 언어 모델(GPT, Claude, Gemini, Llama 등)을 일관된 인터페이스로 연결해 텍스트 생성·분석을 수행하는 핵심 엔진이다.<br>\n",
    "LLM은 AI의 뇌 역할을 하며 LangChain은 이를 체인·에이전트 구조로 확장해 다양한 응용 서비스를 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538201b5-202f-4c5c-9149-2d11cf1eec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일의 내용 불러오기\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bd783-cd95-4078-9443-612ba4696caf",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a5d16a-6c24-4754-9175-953e4c873223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파란 이유는 지구 대기의 산란 현상 때문입니다. 태양광은 여러 색의 빛으로 이루어져 있으며, 각 색의 빛의 파장이 다릅니다. 이 중 파란색 빛은 짧은 파장을 가지고 있어서 대기 중의 공기 분자와 더 많이 산란됩니다.\n",
      "\n",
      "태양빛이 대기에 들어오면, 파란색 빛이 다른 색의 빛보다 더 많이 산란되어 하늘이 파란색으로 보이는 것입니다. 특히, 태양이 수평선 근처에 있을 때는 더 많은 대기층을 통과해야 하기 때문에, 길고 짧은 파장의 색들이 더 많이 섞이고 산란되어 붉은색 또는 주황색의 노을이 생기게 됩니다. 반면, 정오쯤 태양이 높이 있을 때는 직진하는 빛이 많기 때문에 하늘이 더 파랗게 보이게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response= model.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f359d6-5fe2-4fd9-9ac3-7083d6ef91b8",
   "metadata": {},
   "source": [
    "### Anthropic\n",
    "https://docs.claude.com/en/docs/about-claude/models/overview?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef78ad44-f4ff-49e3-a10b-025d60e4edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일에 ANTHROPIC_API_KEY=\"YOUR_API_KEY\" 를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3fb7b7-5b2c-480c-8d13-048ed58f00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-anthropic\n",
    "# notebook Kernel Restart!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b8713a-1709-4a72-bae7-665b0db1b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 하늘이 파란 이유\n",
      "\n",
      "하늘이 파랗게 보이는 이유는 **레일리 산란(Rayleigh scattering)** 때문입니다.\n",
      "\n",
      "## 원리\n",
      "\n",
      "1. **태양빛의 구성**\n",
      "   - 태양빛은 여러 색깔(파장)이 섞인 백색광입니다\n",
      "\n",
      "2. **대기와의 상호작용**\n",
      "   - 태양빛이 대기를 통과할 때 공기 분자와 부딪힙니다\n",
      "   - 파장이 짧은 빛(파란색, 보라색)이 파장이 긴 빛(빨간색)보다 더 많이 산란됩니다\n",
      "\n",
      "3. **파란색이 보이는 이유**\n",
      "   - 보라색이 더 많이 산란되지만, 우리 눈은 파란색에 더 민감합니다\n",
      "   - 산란된 파란 빛이 하늘 전체에서 우리 눈으로 들어옵니다\n",
      "\n",
      "## 다른 시간대의 하늘\n",
      "\n",
      "- **일출/일몰**: 빛이 대기를 더 길게 통과하면서 파란 빛은 다 산란되고, 빨간색/주황색 빛만 우리에게 도달합니다\n",
      "- **우주에서**: 대기가 없어 산란이 일어나지 않아 하늘이 검게 보입니다\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",   # 사용 가능한 버전: opus, sonnet, haiku 등\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 메시지 구성\n",
    "messages = [HumanMessage(content=\"하늘은 왜 파란가요?\")]\n",
    "\n",
    "# 모델 응답\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac235c-3994-4d7d-b6b5-dc97c0e66d8c",
   "metadata": {},
   "source": [
    "### Google Gemini\n",
    "https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885d55f3-b58b-410f-a26a-e351342f3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일에 GOOGLE_API_KEY=\"YOUR_API_KEY\"를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27bb9a4-f761-4769-b10a-356721922cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "702b103b-7e18-4250-93a3-f496a3026947",
   "metadata": {},
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 초기화\n",
    "# model: \"gemini-2.5-pro-latest\" 또는 \"gemini-2.5-flash-latest\", \"gemini-pro\" 등을 사용할 수 있습니다.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") \n",
    "\n",
    "# 메시지 구성\n",
    "prompt = \"안녕하세요! 나는 홍길동입니다.\"\n",
    "\n",
    "# 모델 응답\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a64e2-2d86-4c95-a586-a814bce66eef",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "Ollama는 로컬 환경에서 Llama 3, Mistral, Gemma 등 다양한 LLM을 직접 실행할 수 있게 해주는 오픈소스 플랫폼이다.\n",
    "클라우드 요금 없이 빠르고 보안성 높게 AI 모델을 구동·연동할 수 있다는 점이 가장 큰 장점이다. <br>\n",
    " https://ollama.com/\n",
    " \n",
    "- Llama 3: 성능 최고, 논리·코딩·대화 품질이 가장 우수하지만 모델이 커서 속도는 다소 느림. <br>\n",
    "- Mistral 7B : 성능과 속도의 균형형, 중간 크기 모델로 가볍고 빠르며 추론력도 양호. <br>\n",
    "- Gemma 2 / 3 : ⚡속도 가장 빠름, 작은 환경에 적합하지만 이해력·정확도는 낮음. <br>\n",
    "고성능이 목표라면 Llama 3, 속도·효율을 원하면 Mistral, 저사양 환경이면 Gemma가 가장 적합하다. <br>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e38b2084-cc05-4e2c-a280-0a846a6f6aa5",
   "metadata": {},
   "source": [
    "https://ollama.com/ 에서 OllamaSetup.exe 파일 다운로드 (1.12GB) 설치 \n",
    "Ollama 실행 후 \"gemma3:4b\" 모델 다운로드 후 진행한다 \n",
    "모델 다운로드 경로 :  \"C:\\Users\\<사용자 계정이름>\\.ollama\\models\\blobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f261a8e1-a2e1-4361-9fb8-5bff8a78c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1647ed52-6114-4c5f-b36f-45c73c1de99c",
   "metadata": {},
   "source": [
    "# 모델 경로 변경 방법\n",
    "# 예시 (Windows PowerShell)\n",
    "$env:OLLAMA_MODELS=\"D:\\Ollama_Models\"\n",
    "\n",
    "# 예시 (Linux/macOS)\n",
    "export OLLAMA_MODELS=\"/mnt/storage/ollama_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b60aea2b-d188-413d-b60a-0f35e0d2a2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파랗게 보이는 현상은 ‘레일리 산란(Rayleigh scattering)’이라는 과학적인 현상 때문에 발생합니다. 좀 더 자세히 설명해 드릴게요.\n",
      "\n",
      "**1. 햇빛의 구성:**\n",
      "\n",
      "* 햇빛은 사실 여러 가지 색깔의 빛이 섞여 있는 것입니다. 빨간색, 주황색, 노란색, 초록색, 파란색, 보라색 등 다양한 색깔의 빛이 모두 햇빛 안에 포함되어 있습니다.\n",
      "\n",
      "**2. 레일리 산란:**\n",
      "\n",
      "* 햇빛이 지구 대기에 들어오면, 대기 중의 작은 입자들(질소, 산소 분자 등)과 부딪히게 됩니다.\n",
      "* 이 때 빛이 사방으로 흩어지는 현상을 ‘산란’이라고 합니다.\n",
      "* 빛의 파장이 짧을수록 산란이 더 잘 일어납니다. 파장이 짧은 파란색, 보라색 빛은 빨간색, 주황색 빛보다 훨씬 더 쉽게 산란됩니다.\n",
      "\n",
      "**3. 왜 파란색이 더 잘 보이는가?**\n",
      "\n",
      "* 햇빛이 대기를 통과할 때, 파란색 빛이 다른 색깔의 빛보다 훨씬 더 많이 산란됩니다.\n",
      "* 이렇게 산란된 파란색 빛이 우리 눈에 들어오기 때문에 하늘이 파랗게 보이는 것입니다.\n",
      "\n",
      "**4. 보라색은 왜 보이지 않을까?**\n",
      "\n",
      "* 보라색 빛도 파란색 빛만큼 산란이 잘 되지만, 햇빛 자체에 포함된 보라색 빛의 양이 파란색 빛보다 적고, 우리 눈이 파란색에 더 민감하게 반응하기 때문에 보라색은 하늘에서 덜 느껴지는 것입니다.\n",
      "\n",
      "\n",
      "**5. 일몰/해질녘 하늘은 왜 붉은색/주황색인가?**\n",
      "\n",
      "* 해가 지는 시간에는 햇빛이 대기를 통과하는 거리가 훨씬 길어집니다.\n",
      "* 이 과정에서 파란색 빛은 대부분 산란되어 우리 눈에 도달하지 못하고, 파장이 긴 빨간색, 주황색 빛만이 우리 눈에 들어오게 됩니다.\n",
      "\n",
      "이해가 되셨나요? 혹시 더 궁금한 점이 있다면 언제든지 질문해주세요!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "\n",
    "response= llm.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e5d6cd-0b8d-433f-a24f-6975f397543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파란 이유는 **레일리 산란(Rayleigh scattering)**이라는 현상 때문입니다. 좀 더 자세히 설명해 드릴게요.\n",
      "\n",
      "**1. 햇빛은 여러 색깔의 빛으로 이루어져 있어요.**\n",
      "\n",
      "무지개를 생각해보세요. 햇빛이 빗방울에 굴절되면서 빨주노초파남보 모든 색깔의 빛으로 분리되어 보여주죠. 햇빛은 사실 이 모든 색깔의 빛이 섞여 있는 것입니다.\n",
      "\n",
      "**2. 빛이 공기 중의 작은 입자들과 부딪히면 흩어집니다.**\n",
      "\n",
      "햇빛이 지구 대기를 통과할 때, 공기 중의 질소나 산소와 같은 작은 입자들과 부딪힙니다. 이때 빛은 모든 방향으로 흩어지게 되는데, 이를 '산란'이라고 합니다.\n",
      "\n",
      "**3. 파장이 짧은 빛일수록 더 잘 흩어집니다.**\n",
      "\n",
      "여기서 중요한 건, 빛의 파장에 따라 산란되는 정도가 다르다는 것입니다. 파장이 짧은 빛(파란색, 남색)은 파장이 긴 빛(빨간색, 주황색)보다 훨씬 더 잘 산란됩니다. 마치 작은 돌멩이가 큰 파도보다 더 쉽게 튀는 것과 비슷하다고 생각하면 됩니다.\n",
      "\n",
      "**4. 그래서 하늘은 파랗게 보이는 겁니다.**\n",
      "\n",
      "햇빛이 대기를 통과할 때, 파란색 빛이 다른 색깔의 빛보다 더 많이 산란되어 우리 눈에 들어오기 때문에 하늘이 파랗게 보이는 것입니다.\n",
      "\n",
      "**추가적으로 알아두면 좋은 점:**\n",
      "\n",
      "*   **노을은 왜 붉은색일까요?** 해가 지면 햇빛이 우리 눈에 들어오는 거리가 더 길어집니다. 이 과정에서 파란색 빛은 대부분 산란되어 사라지고, 덜 산란되는 붉은색 빛이 우리 눈에 들어오기 때문에 노을이 붉게 보이는 것입니다.\n",
      "*   **달에서는 하늘이 검은색이에요.** 달에는 대기가 없기 때문에 빛이 산란되지 않아 하늘이 검게 보입니다.\n"
     ]
    }
   ],
   "source": [
    "# Ollama 앱 실행 후 \"gemma3:12b\" 모델 다운로드 후 실행한다\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"gemma3:12b\")\n",
    "\n",
    "response= llm.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)\n",
    "# 응답 오래 걸림 (CPU 사용 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ea622-bd8a-45e5-96d0-9cd9cedb8643",
   "metadata": {},
   "source": [
    "### HuggingFacePipeline\n",
    "HuggingFacePipeline은 LangChain에서 Hugging Face의 Transformer 모델을 직접 호출할 수 있도록 연결해주는 클래스이다.<br>\n",
    "OpenAI API처럼 외부 LLM을 쓰지 않고 로컬 또는 Hugging Face Hub 모델을 바로 사용할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e409f2-b314-48d8-92ae-8bad265187f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch  langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df2c44e-ba08-4ba3-9f07-bfafd991895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Why the sky is blue.\n",
      "\n",
      "\"There is a small group of people who believe the sky is blue and that it is the best view of the day. People who believe that the sky is blue would like to explain the reason they are saying that. They don't want to spend their whole lives trying to find out the truth of this theory. It is clear that the sky is blue and the sky is blue.\"\n",
      "\n",
      "\"This is not about conspiracy theories,\" said the mother who was leading the rally. \"This is about people who are trying to explain why the sky is blue. They want to tell people who disagree with the sky that they are wrong and that they are the ones who are wrong. They need to stop. They need to stop trying to explain why the sky is blue. They should stop calling themselves conspiracy theorists.\"\n",
      "\n",
      "\"I'm just trying to get it right,\" the mother declared. \"I'm trying to get it right before I say things like 'I didn't do anything wrong\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# 1. Hugging Face 파이프라인 생성\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",           # 영어 전용 모델 ,  성능 낮음   \n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7    \n",
    ")\n",
    "\n",
    "# 2. LangChain용 래퍼로 감싸기\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 3. 모델 호출\n",
    "response = llm.invoke(\"Explain Why the sky is blue.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d04810f-4690-4f70-87ea-55ea0f6719d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d8b7065ba84da4b311b263ca732ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace 모델을 다운로드\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID\n",
    "    task=\"text-generation\",          # 수행할 작업을 지정 -> 텍스트 생성\n",
    "                                     \n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# 모델 크기 12.5 GB  , 100 Mbps에서 약 17분 예상, 다운로드 완료 후 재실행시는 캐시에서 읽어서 바로 실행됨\n",
    "# 다운로드 경로 : C:\\Users\\<사용자 계정이름>\\.cache\\huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c724f-ba26-453c-bb6a-8344a2aad603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 호출 (추론)  : # CPU 환경에서 수십분 소요\n",
    "prompt = \"하늘은 왜 파란가요?\"\n",
    "response = hf.invoke(prompt)  # CPU 환경에서 수십분 소요\n",
    "\n",
    "print(\"질문:\", prompt)\n",
    "print(\"모델 응답:\", response)\n",
    "\n",
    "# 일반 데스크탑 (i5, 16GB RAM) 6~8코어 --> 20 ~ 40분\n",
    "# 토큰 생성 속도 초당 0.5~1 토큰n\n",
    "# CUDA 지원 GPU (예: RTX 3060 이상) device_map=\"auto\" 또는 device=0 설정으로 GPU로 전환 → 512토큰 기준 5 ~ 10초 이내 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc097d4-8666-49fb-a6b5-4b969221fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 있을 경우 : device_map=\"auto\" 또는 device=0 으로 설정\n",
    "# GPU 없을경우 : device=-1\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",\n",
    "    task=\"text-generation\",\n",
    "    # device=0,  # GPU 사용\n",
    "    device=-1,  # CPU 사용\n",
    "    # model_kwargs={\"device_map\": \"auto\"},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# 모델 호출 (추론)\n",
    "prompt = \"하늘은 왜 파란가요?\"\n",
    "response = hf.invoke(prompt)\n",
    "\n",
    "print(\"질문:\", prompt)\n",
    "print(\"모델 응답:\", response)\n",
    "\n",
    "# GPU 메모리 부족시 (14.70 GiB 필요) \n",
    "# OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. \n",
    "# Of the allocated memory 14.70 GiB is allocated by PyTorch, and 1.44 MiB is reserved by PyTorch but unallocated. \n",
    "# If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  \n",
    "# See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a038cb27-3025-4d8f-979b-18ef8e1f2567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because you do not have suffient permissions. Please try running as an administrator.\n"
     ]
    }
   ],
   "source": [
    "# CUDA 확인\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e6851c-5b82-4705-94b5-e697582a084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b69bd1-d5bc-4ba6-ad66-e5feede74630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA device count: 0\n",
      "PyTorch build: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"PyTorch build:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8964bbf-41df-4877-9c3a-4b549987bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"EleutherAI/polyglot-ko-1.3b\", # 성능 낮음\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"하늘은 왜 파란가? 그 이유는\")\n",
    "print(response)\n",
    "# 모델 크기 약 2.1 GB  , 100 Mbps에서 약 3~5분 예상, 다운로드 완료 후 재실행시는 캐시에서 읽어서 바로 실행됨\n",
    "# 다운로드 경로 : C:\\Users\\storm\\.cache\\huggingface\n",
    "\n",
    "# Polyglot-Ko-1.3B는 “Instruction (질문-답변)” 모델이 아님\n",
    "# 이 모델은 단순 언어 모델 (Language Model) 입니다\n",
    "# “Q&A 구조”로 학습된 게 아니라 “문장 다음에 올 단어를 예측하는” 형태로 학습됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efe3c1-e29e-446a-b1b9-4f1ce7767ce2",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "GPT4All은 Nomic AI가 개발한 오픈소스 로컬 LLM(Local Large Language Model) 프레임워크이다. <br>\n",
    "이 모델은 인터넷 연결 없이 실행할 수 있으며, GPU 없이도 CPU 기반으로 추론 가능하다는 점이 특징이다. <br>\n",
    "https://www.nomic.ai/gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ba5fe4-cedb-4c3e-9054-be0d3baae0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982e69c1-8b12-48b4-98d3-aebda82837a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sky appears blue because of a process called Rayleigh scattering. When sunlight passes through the Earth's atmosphere, it scatters in all directions, including towards the blue end of the visible spectrum. This scattering causes the blue light to be scattered more than other colors, making the sky appear blue.\n"
     ]
    }
   ],
   "source": [
    "# GTP4All Falcon 모델 : (3.9GB) , 한국어 지원 안됨\n",
    "# 윈도우 메뉴의 GTP4All 프로그램을 실행해서 모델을 미리 다운로드해 놓은 다음 실행\n",
    "# 모델 다운로드 경로:  C:\\Users\\<사용자 계정 이름>\\AppData\\Local\\nomic.ai\\GPT4All\\gpt4all-falcon-newbpe-q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(model=model_path)\n",
    "response = llm.invoke(\"Why is the sky blue?\") # 한국어는 지원 안됨\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bdfe64d-1dda-4f3a-a8d4-8852c35768a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파란색으로 보이는 이유는 대기 중 산소와 질소가 포함된 가스 분자들이 태양광을 흡수하고 재방출하기 때문입니다. 이 과정은 빛의 산란이라고 알려져 있습니다.\n",
      "\n",
      "태양에서 나오는 햇빛은 주로 파랑과 보라색 같은 단파장 색깔로 구성되어 있으며, 이는 공기 입자에 의해 더 잘 산란됩니다. 대기 중 가스 분자들은 태양광을 흡수하고 재방출하는데, 이 과정에서 일부 빛을 다시 지구 표면으로 되돌려 보내게 됩니다.\n",
      "\n",
      "이 빛의 산란은 하늘에 파란색을 부여합니다. 하지만 실제로는 햇빛과 공기 입자 사이의 상호작용 때문에 다양한 색조가 나타납니다. 예를 들어, 대기 중 수증기가 태양광을 흡수하고 재방출하면 하늘이 더 푸른색으로 보일 수 있습니다.\n",
      "\n",
      "하늘이 파란색으로 보이는 이유는 대기 중 산소와 질소가 포함된 가스 분자들이 태양광을 흡수하고 재방출하기 때문입니다. 이 과정은 빛의 산란이라고 알려져 있습니다.는 대기 중 물방울 수가 많아져 햇빛을 더 강하게 산란시켜 하늘을 더욱 선명하게 보이게 합니다.\n",
      "\n",
      "태양에서 나오는 햇빛은 주로 파랑과 보라색 같은 단파장 색깔로 구성되어 있으며, 이는 공기 입자에 의해 더 잘 산란됩니다. 대기 중 가스 분자들은 태양광을 흡수하고 재방출하는데, 이 과정에서 일부 빛을 다시 지구 표면으로 되돌려 보내게 됩니다.\n",
      "\n",
      "이 빛의 산란은 하늘에 파란색을 부여합니다. 하지만 실제로는 햇빛과 공기 입자 사이의 상호작용 때문에 다양한 색조가 나타납니다. 예를 들어, 대기 중 수증기가 태양광을 흡수하고 재방출하면 하늘이 더 푸른색으로 보일 수 있습니다.\n",
      "\n",
      "또한, 지평선 근처의 구름이나 먼지 같은 다른 요소들도 빛 산란에 영향을 줄 수 있으며, 이로 인해 하늘 색깔이 달라질 수 있습니다. 예를 들어, 해질녘과 새벽에는 대기 중 물방울 수가 많아져 햇빛을 더 강하게 산란시켜 하늘을 더욱 선명하게 보이게 합니다.\n"
     ]
    }
   ],
   "source": [
    "# EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf 한국어 지원 모델 : 5.7GB \n",
    "# 윈도우 메뉴의 GTP4All 프로그램을 실행해서 모델을 미리 다운로드해 놓은 다음 실행\n",
    "# 모델 다운로드 경로:  C:\\Users\\<사용자 계정 이름>\\AppData\\Local\\nomic.ai\\GPT4All\\EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 프롬프트 \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#  모델 초기화\n",
    "# model는 GPT4All 모델 파일의 경로를 지정\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=model_path,\n",
    "    # backend=\"gpu\",  # GPU 설정\n",
    "    streaming=True,   # 스트리밍 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백 설정\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 실행\n",
    "response = chain.invoke({\"question\": \"하늘은 왜 파란가요?\"})   # CPU에서 동작 응답 느림\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404a29d-447d-49ef-aa2b-fdd69d8026ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
